# Spark 

## Spark Architecutre
<ol>
<li>Spark Core : Used for monitoring, scheduling and processing the task</li> 
<li>Spark API (Polyglots) : Supports Java, Scala, Python and R</li> 
<li>Spark SQL & Dataframes: Uses SQL queries. DF supports functional programming to solve data crunching problem</li> 
<li>MLib: Used for Machine Learning</li> 
<li>GraphX: Used for graphs</li> 
<li>Streaming APIs: Streaming: Consume and Process a continuous stream of data</li> 
<li>Cluster manager : Spark runs on Yarn, Mesos, Kubernetes, Standalone & Local</li> 
<li>Distributed storage : HDFS, Amazon S3, NoSQL, RDBMS</li> 
</ol>
![Spark Ecosystem](https://github.com/sumitborhade/Spark/raw/master/Spark_EcoSystem.png)

## Advantages
1. Abstraction : Abrastracts distributed architecture and makes feel like working on single machine and executing SQL queries so underlying complexities are hide.
2. Unified processing: Can use polyglot, provides various liberaries to work for Machine Learning & GraphX
3. Ease to use: Complexities are hiden by Spark
4. Fast processing: In memory processing. 10x faster than Hadoop on disc and 100x faster on memory
5. Powerful caching: It has caching management


